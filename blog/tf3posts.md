# Natural Language Processing in TensorFlow

## Тиждень 1

10.05.2023

Завершив перший тиждень курсу "Natural Language Processing in TensorFlow" з циклу DeepLearning.AI TensorFlow Developer. Минулі курси були про застосування TensorFlow для машинного навчання та використання моделей при класифікації зображень. Цей курс, як свідчить з назви, про навчання моделей на текстах для їх подальшого використання в аналізі та прогнозуванні. Я, якщо чесно, не знаю багато прикладів застосувань таких моделей. На думку спадає використання класифікації текстів, розумного пошуку, чи щось такого. В програмі курсу обіцяють, що навчимо моделі творити поезію. Ну і ця тема, мабуть напряму стосується ChatGpt.      

Цього тижня вчилися перетворювати тексти в цифрову форму. Важливими є саме слова, а не літери. Слова перетворюються в цілі числа (токенізуються), з яких формуються словники, які до того ж доповнюються службовим словом і числом "невідомий". Далі масивами (кортежами) з таких чисел кодуються речення в текстах, які вирівнюються до довжини найдовшого речення, а все інше заповнюється нулями. Вот і все, що вивчалося. Звісно це все робиться дуже просто через бібліотеку keras.preprocesing із TensorFlow. На лабах це все тестували (тикали виконувати і дивилися на пояснення і результати). Оцінювання тестів здебільшого було в дусі "як називається функція, яка робить тето ...".

Самостійна практична на оцінку не обійшлася без новинок. Виявилося, що в реальних текстах приходиться викидувати "стопові" слова - службові слова, які практично не впливають на зміст речень, і які не варто токенізувати. Про такий прийом було буквально сказано одне речення посеред завдання на лабу, може далі буде більш детально про це сказано. Кілька десяток таких слів давалися готовим набором (кортежем), і вихідні тексти треба було "почистити".  Мене, власне, здивувало з половина слів в цьому списку, ніколи б не назвав їх службовими, наприклад "all". Отже в лабі треба було завантажити csv файл, в якому кожен рядок представляв собою дві частини - маркер статті і сама стаття, які розділені комою (в самій статті з розділових знаків тільки пробіли і крапки). Треба було почистити тексти статей від службових слів і токенізувати обидві частини.  https://www.coursera.org/learn/natural-language-processing-tensorflow

## Тиждень 2

14.05.2023 

Традиційно звіт по моєму навчанню в циклі DeepLearning.AI TensorFlow Developer. Отже заврешив другий тиждень "Natural Language Processing in TensorFlow"  https://www.coursera.org/learn/natural-language-processing-tensorflow. Вже почали навчати моделі на класифікацію бінарну - позитивний/негативний відгук по фільмам, і множинну - на самостійному завданні. 

Спочатку вчилися використовувати техніку Embedding (вбудовування). Я не врубився з пояснень в курсі як це працює. Наче і пояснювали і візуалізували, але як воно представлено в середині, я з курсу так і не зрозумів. ChatGpt тут також мені не сильно допоміг. Тобто він сказав мені практично те саме, типу перетворивши кожне слово у вектор мені дасть можливість робити їх схожими, цитую:  `Якщо слова мають подібні значення, то їх вектори будуть розташовані близько один до одного в просторі. Наприклад, слова "cat" і "dog" можуть бути близькими векторами в просторі, оскільки вони відносяться до  тварин, а слово "computer" може бути далеким від них, оскільки не має  спільного значення.` Не зміг я це осмислити до кінця (візуалізувати в голові таку векторність), тому вирішив старим добрим способом погуглити, але з використанням перегляду зображень. І наткнувся на курс від Гугла "Прискорений курс по машинному навчанні". Ця лекція по Embedding мені реально зайшла, тому рекомендую https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture . До речі, треба дивитися в хромі, на файрофксі некоректно відображається. Крім того, я скористався гугловським машинним перекладом, і мені зайшло на ура. Рекомендую переглянути лекцію, але я залишу тут примітку для себе. В лекції розглядався кейс з фільмами, які необхідно порекомендувати переглянути в залежності від відгуків. Тобто, якщо тобі зайшов якийсь фільм, то інший фільм яким віддали переваги так само тобі також зайде. Кількість критеріїв означує кількість вимірів, а отже і кількість оцінок в певному діапазоні. Чим більш схожі будуть ці числа, тим більш схожі ці фільми відповідно до вибраних критеріїв. І дуже важливо, що саме по відношенню до конкретної задачі вони будуть схожі, тому це буде ще один шар в моделі. Якщо мепити це на слова, то саме слово отримує контекст (вектор), коли воно зв'язане з іншими словами, а сильна схожість таких контекстів є по суті синонімами по відношенню до задачі навчання. Може використовуватися для різних типів даних - текстів, аудіо, зображень щоб знайти в них подібність. Десь так.     

Оглянули Tensor Flow Data Service де існує багато різних наборів даних (датасетів) для різних типів, серед яких юзали датасети відгуків IMDB. З нього витягнули марковані відгуки, токенізували, як вчили минулого тижня, і побудували модель, в який в якості вхідного шару вставили вже згаданий вище, вбудований шар. Ну і це все попробували на лабі. Результат вбудовування подивилися на TF Embedding Projector (https://projector.tensorflow.org/) де цю близькість слів можна візуалізувати за експортними даними. 

Далі розглядали заміну другого звичайного шару Flatten, на глобальне середнє об’єднання 1D  (global average pooling 1D) для зменшення розмірності. Якщо по ембедінг намагалися роз'яснити, то пояснення щодо цього шару обійшлося одним реченням. Я намагався вникнути в математику, але покинув цю ідею ("не на часі"). Друга лаба по факту була така як перша, але для датасетів по "сарказму" з минулого тижня. Додатково розглянули як Гіперпараметри, такі як розмір словника, розмірність вбудовування та максимальна довжина речення впливає на навчання моделі.  

Остання частина для мене була найцікавішою. Тут вже розглядався підхід навчання вже не по маркуванню цілих слів, а їх частин. Ідея в тому, що усі слова складаються з частин і саме вони маркуються (позначаються окремим числом і записуються в словник). Далі усі речення кодуються такими маркерами. Усе інше - як зазвичай. Перевага такого підходу в тому, що для великих наборів даних словник буде меншим, і "невизначених" слів може взагалі практично не бути. Точність моделі вище, але ще є куди рости, бо як мінімум не відстежується місце в слові та реченні. На лабах використовували вже напередтокенізовані набори даних, що базуються на підсловах, і аналізували на скільки стало краще порівняно з попереднім підходом.       

Щодо тестування - були питання по типу "як назвиається", і на попередню тему, і на уважність читання і розуміння. На останнє питання відповіді у лекціях і лабораторках не було (принаймні я не помітив), тому вибрав з точки зору логіки варіанту відповіді і вгадав. У будь якому випадку черговий раз відмічу, що сам процес відповіді на запитання вносить вагомий вклад у вивчення матеріалу. Це якраз той випадок, де самооцінювання важливіше ніж оцінювання на сертифікацію. 

У самостійній на оцінку доробляли минуле завдання на множинну класифікацію новин BBC. Зрештою, хоч нічого позаштатного робити не приходилося, треба розуміти що ти робиш, простим копіпастом не обійдешся. Ну, тобто приходиться копіпастити складно, з кількох минулих робіт. Треба згадати те, що ти робив на 2-му курсі. 

## Тиждень 3

25.05.2023

Пройшов заняття третього тижня  "Natural Language Processing in TensorFlow"  https://www.coursera.org/learn/natural-language-processing-tensorflow із циклу  DeepLearning.AI TensorFlow Developer. На цьому тижні було багато експериментів з різними будовами мереж. На лабах і лекціях усі вони показали, що модель на перевірочних даних показує перенавчання практично після перших епох. Графіки точності перевірки невпинно падали, а втрати невпинно зростали з кожною епохою буквально після першої або другої, причому у всіх варіантах. Згідно результатів з лекцій і лаб сама проста мережа, як на мене, дала самі кращі результати. Тепер трохи детально.

Почали з розгляду основ рекурентних нейронних мереж (RNN). Як і до цього під капот технології не заглядували, зате є посилання на відео від "класика" Ендрю, де все математично розписано. Я, відверто кажучи, не дивився, бо не мав такої цілі. Так, от при навчанні моделі для класифікації текстів важливо не тільки набір слів в реченні, але і їх послідовність. RNN якраз передбачають зв'язки між "горизонтальними" елементами (комірками), наприклад словами в реченні, які стоять поруч. Але логічно зв'язані елементи далеко не завжди стоять поряд в реченнях, тому вдосконаленим варіантом RNN є LSTM (long short - term memory), який передає контекст через додатковий конвеєр на рівні клітини (наприклад речення). Контекст через конвеєр варто направляти в обидва боки (двонаправлений) що робиться через спеціальний метод тензор флов - Bidirectional, а кількість шарів  LSTM дає можливість організовувати конвеєри контекстів на різних рівнях абстракції, наприклад перші - на реченні, наступні - на абзацах. Крім того, кілька таких шарів показують більш стабільні результати на перевірочних даних. Як виявилося, для даних задач можна також задіювати згорткові рівні, які використовувалися в курсі раніше для класифікації зображень. ChatGPT сказав, що згортки у даному випадку можуть ефективно відловлювати послідовності слів та шаблони.   

На лабах гралися з різними варіантами організації рівнів, які описав вище. Додатково також був експеримент з рівнем реалізованим на дещо спрощеному варіанті LSTM - GRU (Gated Recurrent Unit). Результати лабораторних я описав напочатку "звіту". Коментар від автора про такий ефект перенавчання - частина слів в перевірочних датасетах просто відсутні в словнику. Крім того, у попередньому тижні використовували частини слів, а в цьому - знову цілі слова. Із-за цього залишилась якась непонятка, можливо розсіється наступного тижня.

Тестування було цікавим. Особливо потішило останнє питання, яке дуже резонує з моїм початком поста :) Серед питань було тільки одне питання по типу "Як називається метод ....". Усі викладачі викручуються однаково, коли більше нічого не лізе в голову, або треба більше позитивних результатів для "легкості" проходження курсу :)  

Тестове завдання складне у зв'язку з великою тривалістю навчання, бо кількість епох - 20 і це не можна замінити. Я приготовився, що сьогодні буду весь день вчити модель, щоб вона задовольняла вказаним вимогам. Уже намітив паралельну задачу, між якими буду переключатися. Але мені пощастило (мабуть).  Треба уважно почитати підказки. Я прогнав дві однакові моделі без і з використанням рівня Dropout (випадкове просіювання). І з другого разу модель стала задовольняти. Різниця між результатами просто вражаюча, просіювання реально рулить. Яку модель вибрав не буду казати, бо не можна згідно правил. Але у мене враження що усі запропоновані згодяться, я полінувався перевіряти. 

Ще один тиждень і далі четвертий курс, ради якого я пішов вчитися на цей напрям. Тому далі буде.    

## Тиждень 4

29.05.2023

Пройшов заняття четвертого і останнього тижня  "Natural Language Processing in TensorFlow"  https://www.coursera.org/learn/natural-language-processing-tensorflow із циклу  DeepLearning.AI TensorFlow Developer.  На цьому тижні розглядали використання моделей для прогнозування текстів. Ідея в тому, що якщо модель вчиться на певних наборах текстів, то можна прогнозувати наступні слова (або частини слів) за попередньою існуючою частиною. Причому цей прогноз буде опиратися саме на ті тексти, на основі яких вчилася модель. Якщо це були твори Шекспіра, то і стиль отриманого тексту буде як у Шекспіра. Таким чином прогнози можна видати за нові твори. А тепер трохи деталей.

Отже якщо у навчальних текстах часто зустрічаються фрази "Русскій ваєнний карабль - іді ...", то за першими трьома словами, які будуть входами моделі, можна отримати наступні два, які будуть маркерами.    Робиться це шляхом формування з кожного окремого речення вихідного тексту не одного а кількох послідовностей (n-грам підречень), кожне з яких складається тільки з частини речення, яке має вхідні дані та маркер. Вхідні дані будуть лівою частиною речення, а маркер - словом, яке йде за ними. У нашому випадку, наприклад перша послідовність це буде "русскій", а маркер "ваєнний"; друга послідовність - "русскій ваєнний" а маркер - "карабль" і т.д. Далі за сформованим набором маркерів (токенів) формується масив, який буде давати одиницю у потрібному індексі маркеру. Таким чином задача передбачення перетворюється у задачу множинної класифікації. 

Однак, як завжди, не все так просто. Кожне нове слово в послідовності приводить до зменшення ймовірності до пошуку вдалого слова, особливо коли навчальний набір (корпус) був не достатньо великим. Звідси згенеровані тексти виглядають якоюсь нісенітницею, з частими повторами слів.         

На лабораторних роботах гралися з передбаченнями. Мені важко сказати наскільки вдалими вони були, бо моделі навчалися за ірландськими піснями і генерували відповідні пісні. Остання лабораторна була переглядом коду для трохи іншого підходу, де в якості послідовностей використовуються символи а не слова, тобто модель базується на символьно-орієнтованих рекурентних нейронних мережах RNN. Цей підхід більш гнучкий, за тими ж причинами які розглядалися на другому тижні при навчанні на частинах слів. 

В тестах не було нічого особливого, хоча я був не дуже уважним щодо читання питань і відповідей, тому сотню не вибив. Щодо тестового практичного завдання прийшлося трохи попотіти щодо підбору параметрів, щось нагадало мені підбір параметрів для ПІД регулятора. Як виявилося я просто перемудрив бо подивився не на той приклад. Це як аналогія, коли ти вибрав замість ПІ - ПІД. Сама програма нормально кусками копіпаститься з лабораторок.  

Курс завершено, сертифікат отримано https://www.coursera.org/account/accomplishments/verify/MPQALG2RDXRX . Попереду основна ціль цієї спеціалізації - Sequences, Time Series and Prediction. Вже працюю. 
