# Sequences, Time Series and Prediction

## Тиждень 1

01.06.2023

Нарешті я добрався до 4-го заключного курсу із циклу DeepLearning.AI TensorFlow а саме Sequences, Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction і сьогодні пройшов перший тиждень. Цього тижня знайомилися з часовими рядами, вчилися генерувати синтетичні дані для того щоб використовувати їх при аналізі і краще розуміти властивості часових рядів, такі як сезонність, тенденція (тренд, властивість закономірної зміни) і наявність шумів. Також робили прогнозування на основі простого числового аналізу даних. Далі трохи деталей.

Почали означення поняття часового ряду (одновимірного та багатовимірного) та з області застосування : прогнозування - що буде після, імпутація - що було до і в неозначених проміжках часу, виявлення аномалій, виявлення закономірностей які породжують ряд (наприклад розпізнавання звукового мовлення). Далі знайомилися з ознаками часових рядів: сезонність, тренд та білий шум. Познайомилися з поняттями автокореляції (повторюваність поведінки в різні моменти часу), затримками та інноваціями (випадковими всплесками). Таким чином задача машинного навчання - створити модель, яка би виявила закономірності, звісно без моделювання стохастичної (випадкової) складової. Далі розглянули таку можливу особливість нестаціонарних часових рядів, як кардинальна зміна поведінки після якоїсь значної події. Для таких часових рядів при навчанні замість всіх наявних даних варто використовувати тільки останній проміжок часу (навчальний період), що кардинально відрізняється від попередніх підходів навчання, в яких чим більше даних, тим краще. 

На першій лабораторній роботі генерували та будували часові ряди. Спочатку трохи зосередилися на функціях побудови графіків (pyplot), бо до цього їх просто юзали. Далі побудували пряму залежності від часу, яка, як виявилося, представляє примітивний часовий ряд - тренд. Генерували сезонні ряди, білі шуми, автокореляційну функцію з імпульсами та нестаціонарний часовий ряд.

Далі розбирали яким чином весь набір даних розділити на навчальний, валідаційний та тестовий період. При цьому, як виявилося, від останнього часто відмоволяються і в даному курсі також його не використовують. Для рядів, що мають сезонність, ці частини повинні бути кратні сезонності.  Далі познайомилися з метриками вимірювання ефективності прогнозування та їх області застосування: абсолютна похибка (відхилення), середнє квадратичне відхилення (mse), корінь середньоквадратичного відхилення(rmse), середнє абсолютне відхилення (mae, яке має перевагу над mse коли не треба збільшувати чутливість в залежності від величини відхилення), середнє приведене до шкали значення відхилення (mape). Ці метрики застосували для аналізу "наївного прогнозування" - методу, при якому вважається що наступне значення буде таке ж як попереднє. Як виявилося його використовують як нижню (погану) межу для порівняння метрик з іншими методами прогнозування. Дуже довго не міг повірити, що такий метод існує, бо нагадує мені прогнозування брежнівського періоду в срср. 

Далі розглянули прості методи прогнозуання на основі аналізу. Ковзне середнє - коли усереднюється значення за певний період (вікно усереднення) і використовується в якості прогнозованого значення, що добре підходить для боротьби з шумами (як фільтр), але не враховує сезонності та тренду та може давати навіть гірші прогнози, ніж наївне прогнозування. Тому для врахування сезонності беруть різницю між значеннями в однакових зсувах відносно початку різних часових періодів, що кратні сезонам. Далі, використовуючи ковзне середнє прогнозують поведінку такого різницевого часового ряду. Далі отримані дані додають до значення останнього періоду, пропустивши той попередньо через такий же фільтр ковзного середнього, і отримують набагато кращі прогнози. На другій лабі це все досліджували.          

При оцінюваному тестуванні для мене підступними стали питання про вибір правильного прикладу одновимірного та багатовимірного часового ряду, саме на останньому я помилився. Відповіді правильної так і не знаю (знаю тільки неправильну), бо полінувався другий раз проходити тест для підвищення оцінки. Оцінювана практична легко робиться по аналогії з останньою лабою.

І на заключення, давно хотів про це написати. Останнім часом мої пости на цю тему стають більшими і детальнішими. Це пов'язано з тим, що я готую першу частину такого звіту ще до складання тесту. Це допомагає краще зосередитися на пройденому матеріалі. Це такий собі викладацький підхід, при якому коли хочеш в чомусь розібратися, починай це викладати. Крім того я веду конспект, в який копіпащу тексти з відео з їх перекладами та скрінами презентації. По перше, це дає можливість вчитися на Курсері не маючи достатнього рівня англійської мови для сприйняття її на слух. По друге, це дає можливість швидко пройтися повторно по пройденому матеріалу, навіть після тривалого проходження курсу, для чого зрештою конспекти і існують. Гарна практика, особливо для таких забувак як я.     

## Тиждень 2 

09.06.2023

Завершив другий тиждень курсу Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction. Цього разу вже навчали просту нейрону мережу для прогнозування часових рядів.

Спочатку почали з того, що визначили що в часових рядах є ознаками а що мітками. Ідея в тому, що весь часовий ряд (усі наявні виміри) представляється як набір згрупованих даних які поміщаються в одне вікно. Це вікно має розмір, який задає кількість значень в ньому. Тоді уся послідовність чисел у цьому вікні будуть ознаками, а наступне в часі за вікном значення буде міткою. Це щось схоже на прогнозування слів в реченнях, які розглядалися в минулому курсі. Датасет (набір даних) по факту будуть усі значення що беруться з вікна, яке "пересувається" на одну точку виміру далі з кожною ітерацією, поки не "впреться" в кінець діапазону. Тобто, якщо розмір вікна будо 10 точок, і вікно переміщується на один вперед, то наступна частина датасета буде містити 9 точок попереднього і мітку з попередньої ітерації. На першій лабі вчилися (дивилися) як з використанням tensorflow створювати такі датасети з ряду цілих значень, де обирали розмір вікна, виділяли мітки, перемішували вікна в датасетах для кращого навчання. Далі це робили на синтезованих даних і розділяли дані на навчальні та перевірочні. 

Після ознайомлення з підготовкою датасетів, вчилися прогнозувати з використанням одношарової нейронної мережі з одним виходом, та лінійною активаційною функцією (якщо не вказується при виклиці то по замовченню). Та познайомилися з швидкістю навчання оптимізатора, який задає коефіцієнт масштабування розміру кроку зміни ваг. На лабах дивилися резульати, а також що собою представляють ваги. Це два масиви, один - для кожного входу, другий - для зсуву. Тобто це стандартна лінійна регресія.      

Наступним кроком було розроблення тришарової моделі, в якій середній (прихований) шар мав на виході функцію активації ReLU (замінює на 0 від'ємні значення). Крім того задіяли функцію зворотного виклику яка з кожною епохою збільшувала швидкість навчання. За допомогою цього підходу можна визначити оптимальне значення швидкості навчання, щоб з одного боку підвищити швидкість а з іншого не збільшувати втрати. Як висновок - така мережа краща за одношарову, але в ній прогнозоване значення сильно залежить від точок безпосередньо перед ним, і це можна виправити за допомогою рекурентних нейронок, які будуть розглядатися у наступному тижні.      

На тестуванні використовувалися питання до теми з попереднього тижня. Питання були трішки з підвохом. Оцінюване практичне заняття було легким (копіпаст з лаб), але варто звернути увагу, що на кроці прогнозування Jupiter наче підвисає, треба почекати поки він виконає всі розрахунки, тоді як видимого виконання немає.

## Тиждень 3

17.06.2023

Завершив третій тиждень курсу Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction. Цього разу вже навчали нейрону мережу для прогнозування часових рядів з використанням RNN і LSTM з якими знайомилися ще в попердньому курсі. Як я вже зазначав раніше, хоч спеціалізація має чотири курси, вони йдуть підряд один за одним і базуються на попередніх отриманих знаннях. Почали з повторення призначення RNN, але вже в контексті часових рядів. Нагадаю, що ідея RNN в тому, щоб передавати контекст від однієї клітинки в шарі, яка відповідає за обробку одного вхідного атрибуту на іншу по горизонталі клітинку. Таким чином враховується сама послідовність, яка не є простою сукупністю вхідних ознак а саме послідовністю, де наступний атрибут залежить від попередніх. Тому по факту шар обробляє послідовно один тип комірки, яка має один зовнішній вхід і вихід, один горизонтальний вхід та вихід. Використовуючи кілька таких шарів можна значно покращити модель. Враховуючи що комірка має кілька нейронів і обробляє дані партіями то виходи будуть матрицею. У простому випадку горизонтальні виходи комірок будуть такі самі як і зовнішні. При цьому для зовнішніх шарів RNN, типово тільки останній зовнішній вихід передається назовні (sequence to vector). Далі розглянули Лямбда шари в tf.keras - це шар, який обробляється користувацькою функцією. У прикладах він використовувався для приведення вхідної розмірності до потрібної а також для масштабування у вихідному шарі моделі. Наступним кроком були експерименти з використання LSTM, який передає стан через всі комірки а не тільки між сусідніми. Причому в експериментах використовували три шари ще і двонаправлені (що для мене здалося нелогічним) і отримали кращі результати ніж RNN. Тести допомогли зосередитися на деяких питаннях, які опустив в теорії. Програма на оцінювання була проста, якщо аналізували роботу лабораторок.  