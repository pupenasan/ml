# Sequences, Time Series and Prediction

## Тиждень 1

01.06.2023

Нарешті я добрався до 4-го заключного курсу із циклу DeepLearning.AI TensorFlow а саме Sequences, Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction і сьогодні пройшов перший тиждень. Цього тижня знайомилися з часовими рядами, вчилися генерувати синтетичні дані для того щоб використовувати їх при аналізі і краще розуміти властивості часових рядів, такі як сезонність, тенденція (тренд, властивість закономірної зміни) і наявність шумів. Також робили прогнозування на основі простого числового аналізу даних. Далі трохи деталей.

Почали означення поняття часового ряду (одновимірного та багатовимірного) та з області застосування : прогнозування - що буде після, імпутація - що було до і в неозначених проміжках часу, виявлення аномалій, виявлення закономірностей які породжують ряд (наприклад розпізнавання звукового мовлення). Далі знайомилися з ознаками часових рядів: сезонність, тренд та білий шум. Познайомилися з поняттями автокореляції (повторюваність поведінки в різні моменти часу), затримками та інноваціями (випадковими всплесками). Таким чином задача машинного навчання - створити модель, яка би виявила закономірності, звісно без моделювання стохастичної (випадкової) складової. Далі розглянули таку можливу особливість нестаціонарних часових рядів, як кардинальна зміна поведінки після якоїсь значної події. Для таких часових рядів при навчанні замість всіх наявних даних варто використовувати тільки останній проміжок часу (навчальний період), що кардинально відрізняється від попередніх підходів навчання, в яких чим більше даних, тим краще. 

На першій лабораторній роботі генерували та будували часові ряди. Спочатку трохи зосередилися на функціях побудови графіків (pyplot), бо до цього їх просто юзали. Далі побудували пряму залежності від часу, яка, як виявилося, представляє примітивний часовий ряд - тренд. Генерували сезонні ряди, білі шуми, автокореляційну функцію з імпульсами та нестаціонарний часовий ряд.

Далі розбирали яким чином весь набір даних розділити на навчальний, валідаційний та тестовий період. При цьому, як виявилося, від останнього часто відмоволяються і в даному курсі також його не використовують. Для рядів, що мають сезонність, ці частини повинні бути кратні сезонності.  Далі познайомилися з метриками вимірювання ефективності прогнозування та їх області застосування: абсолютна похибка (відхилення), середнє квадратичне відхилення (mse), корінь середньоквадратичного відхилення(rmse), середнє абсолютне відхилення (mae, яке має перевагу над mse коли не треба збільшувати чутливість в залежності від величини відхилення), середнє приведене до шкали значення відхилення (mape). Ці метрики застосували для аналізу "наївного прогнозування" - методу, при якому вважається що наступне значення буде таке ж як попереднє. Як виявилося його використовують як нижню (погану) межу для порівняння метрик з іншими методами прогнозування. Дуже довго не міг повірити, що такий метод існує, бо нагадує мені прогнозування брежнівського періоду в срср. 

Далі розглянули прості методи прогнозуання на основі аналізу. Ковзне середнє - коли усереднюється значення за певний період (вікно усереднення) і використовується в якості прогнозованого значення, що добре підходить для боротьби з шумами (як фільтр), але не враховує сезонності та тренду та може давати навіть гірші прогнози, ніж наївне прогнозування. Тому для врахування сезонності беруть різницю між значеннями в однакових зсувах відносно початку різних часових періодів, що кратні сезонам. Далі, використовуючи ковзне середнє прогнозують поведінку такого різницевого часового ряду. Далі отримані дані додають до значення останнього періоду, пропустивши той попередньо через такий же фільтр ковзного середнього, і отримують набагато кращі прогнози. На другій лабі це все досліджували.          

При оцінюваному тестуванні для мене підступними стали питання про вибір правильного прикладу одновимірного та багатовимірного часового ряду, саме на останньому я помилився. Відповіді правильної так і не знаю (знаю тільки неправильну), бо полінувався другий раз проходити тест для підвищення оцінки. Оцінювана практична легко робиться по аналогії з останньою лабою.

І на заключення, давно хотів про це написати. Останнім часом мої пости на цю тему стають більшими і детальнішими. Це пов'язано з тим, що я готую першу частину такого звіту ще до складання тесту. Це допомагає краще зосередитися на пройденому матеріалі. Це такий собі викладацький підхід, при якому коли хочеш в чомусь розібратися, починай це викладати. Крім того я веду конспект, в який копіпащу тексти з відео з їх перекладами та скрінами презентації. По перше, це дає можливість вчитися на Курсері не маючи достатнього рівня англійської мови для сприйняття її на слух. По друге, це дає можливість швидко пройтися повторно по пройденому матеріалу, навіть після тривалого проходження курсу, для чого зрештою конспекти і існують. Гарна практика, особливо для таких забувак як я.     

## Тиждень 2 

09.06.2023

Завершив другий тиждень курсу Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction. Цього разу вже навчали просту нейрону мережу для прогнозування часових рядів.

Спочатку почали з того, що визначили що в часових рядах є ознаками а що мітками. Ідея в тому, що весь часовий ряд (усі наявні виміри) представляється як набір згрупованих даних які поміщаються в одне вікно. Це вікно має розмір, який задає кількість значень в ньому. Тоді уся послідовність чисел у цьому вікні будуть ознаками, а наступне в часі за вікном значення буде міткою. Це щось схоже на прогнозування слів в реченнях, які розглядалися в минулому курсі. Датасет (набір даних) по факту будуть усі значення що беруться з вікна, яке "пересувається" на одну точку виміру далі з кожною ітерацією, поки не "впреться" в кінець діапазону. Тобто, якщо розмір вікна будо 10 точок, і вікно переміщується на один вперед, то наступна частина датасета буде містити 9 точок попереднього і мітку з попередньої ітерації. На першій лабі вчилися (дивилися) як з використанням tensorflow створювати такі датасети з ряду цілих значень, де обирали розмір вікна, виділяли мітки, перемішували вікна в датасетах для кращого навчання. Далі це робили на синтезованих даних і розділяли дані на навчальні та перевірочні. 

Після ознайомлення з підготовкою датасетів, вчилися прогнозувати з використанням одношарової нейронної мережі з одним виходом, та лінійною активаційною функцією (якщо не вказується при виклиці то по замовченню). Та познайомилися з швидкістю навчання оптимізатора, який задає коефіцієнт масштабування розміру кроку зміни ваг. На лабах дивилися резульати, а також що собою представляють ваги. Це два масиви, один - для кожного входу, другий - для зсуву. Тобто це стандартна лінійна регресія.      

Наступним кроком було розроблення тришарової моделі, в якій середній (прихований) шар мав на виході функцію активації ReLU (замінює на 0 від'ємні значення). Крім того задіяли функцію зворотного виклику яка з кожною епохою збільшувала швидкість навчання. За допомогою цього підходу можна визначити оптимальне значення швидкості навчання, щоб з одного боку підвищити швидкість а з іншого не збільшувати втрати. Як висновок - така мережа краща за одношарову, але в ній прогнозоване значення сильно залежить від точок безпосередньо перед ним, і це можна виправити за допомогою рекурентних нейронок, які будуть розглядатися у наступному тижні.      

На тестуванні використовувалися питання до теми з попереднього тижня. Питання були трішки з підвохом. Оцінюване практичне заняття було легким (копіпаст з лаб), але варто звернути увагу, що на кроці прогнозування Jupiter наче підвисає, треба почекати поки він виконає всі розрахунки, тоді як видимого виконання немає.

## Тиждень 3

17.06.2023

Завершив третій тиждень курсу Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction. Цього разу вже навчали нейрону мережу для прогнозування часових рядів з використанням RNN і LSTM з якими знайомилися ще в попердньому курсі. Як я вже зазначав раніше, хоч спеціалізація має чотири курси, вони йдуть підряд один за одним і базуються на попередніх отриманих знаннях. Почали з повторення призначення RNN, але вже в контексті часових рядів. Нагадаю, що ідея RNN в тому, щоб передавати контекст від однієї клітинки в шарі, яка відповідає за обробку одного вхідного атрибуту на іншу по горизонталі клітинку. Таким чином враховується сама послідовність, яка не є простою сукупністю вхідних ознак а саме послідовністю, де наступний атрибут залежить від попередніх. Тому по факту шар обробляє послідовно один тип комірки, яка має один зовнішній вхід і вихід, один горизонтальний вхід та вихід. Використовуючи кілька таких шарів можна значно покращити модель. Враховуючи що комірка має кілька нейронів і обробляє дані партіями то виходи будуть матрицею. У простому випадку горизонтальні виходи комірок будуть такі самі як і зовнішні. При цьому для зовнішніх шарів RNN, типово тільки останній зовнішній вихід передається назовні (sequence to vector). Далі розглянули Лямбда шари в tf.keras - це шар, який обробляється користувацькою функцією. У прикладах він використовувався для приведення вхідної розмірності до потрібної а також для масштабування у вихідному шарі моделі. Наступним кроком були експерименти з використання LSTM, який передає стан через всі комірки а не тільки між сусідніми. Причому в експериментах використовували три шари ще і двонаправлені (що для мене здалося нелогічним) і отримали кращі результати ніж RNN. Тести допомогли зосередитися на деяких питаннях, які опустив в теорії. Програма на оцінювання була проста, якщо аналізували роботу лабораторок.  

## Тиждень 4

24.07.2023

Завершив останній тиждень останнього курсу Time Series and Prediction https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction в сертифікатній програмі DeepLearning.AI TensorFlow Developer. Отримав сертифікат https://www.coursera.org/account/accomplishments/verify/TFEPMY6E4LEW . Цього тижня займалися прогнозуванням активності сонячних плям, навчаючи модель вже не на синтетичних а реальних даних. 

Почали з переформатування моделі з використанням згорток, які ще вивчали на першому з курсів для розпізнавання зображень https://pupenasan.github.io/ml/blog/tf1posts.html. Це має сенс, так як нас цікавлять поведінкові паттерни а не самі значення. Крім того це зменшує розмірність. Потім спробували змінювати розмір пакету (batch), що також вплинуло на якість тренованої моделі. Я це наразі точно ніяк не можу пояснити, тому що тільки недавно зрозумів, що тренувальні дані об'єднуються в batch для зменшення часу виконання обчислень на графічних процесорах. Як це пов'язано саме з якістю - зараз зрозуміти не можу. 

Далі оперували реальними даними кількості зафіксованих сонячних плям за місяць в 1749-2018 роках. За цими даними тренували модель з різними структурами, починаючи від класичної тришарової нейронки до використання LSTM та Conv1D,  а також з різними параметрами та кількістю нейронів. Гралися з розміром вікна який, як виявилося, давав гірші результати при логічному виборі з урахуванням сезонності. З чого я зробив висновок, що підбір кращих параметрів далеко не завжди пов'язано з логікою.        

Тести були легкі, але одне питання про сезонність було підступним, як я і підозрював, саме за нього не вибив максимум.  Якщо будете проходити, то враховуйте що відповідь тре давати не про дані сезонності у прикладах, а ті, про які кажуть вчені (;-)). Більшість питань було про те, як виглядить інструкція в Пайтон для ... , але дещо спитали і з минулих тижнів. Треба взяти собі у практику вприскування в тести за розділ питання з попередніх тем, це реально заставляє повторювати матеріал з попередніх розділів. Практична робота на оцінювання була досить простою, прогнозували мінімальну добову температуру в Мельбурні. Я за другий раз натренував модель на необхідні вимоги (копіпаст і уважне читання).

**Загальні підсумки за всіма курсами**

Вважаю конче необхідним пройти цей курс усім категоріям людей, які хочуть зрозуміти як відбувається побудова, навчання та використання моделі для прогнозування за наявними даними, але зовсім не мають такого досвіду. Це такий собі швидкий старт, щоб спробувати як працює машинне навчання, але без занурення в тему і навіть розуміння багатьох основ. Люди, які хоч якось вміють програмувати, уважно читають матеріал, конспектують, легко зможуть пройти усі чотири курси і отримати сертифікат. Тим не менше, як на мене, цей курс явно недостатній для того, щоб професійно займатися ML одразу після нього. Про це пише навіть сам автор цих курсів і посилається на більш глибокі курси від Ендрю, які дійсно є фундаментальними.  

Щодо конкретно моїх потреб, як викладача та розробника, висловлю своє бачення, яким чином можна покращити даний курс. По-перше, треба б до нього принаймні ще якийсь конспект лекцій. Створенням подібного ми з колегами вже почали займатися https://pupenasan.github.io/ml/books/TheLittleBookofDeepLearning/. По-друге, думаю варто його переформатувати під специфічне застосування і доповнити необхідним матеріалом. Наприклад, мене цікавив перш за все останній курс - прогнозування за часовими рядами. Можливо для цієї теми можна взяти за основу послідовність усієї програми і звести до одного напрямку. По-третє, зробити якийсь один кейс - від початку (збір даних) до кінця - прогнозування в системах керування. Цим і плануємо займатися з колегами найближчим часом. Це стане в основу курсу, який читається магістрам на нашій кафедрі і в подальшому думаю можна буде його давати спеціалістам-автоматникам в навчальних центрах. Так, плани дуже амбітні, але у той же час дуже конкретні. Тому все буде зроблено рано чи пізно.

На завершення хочу подякувати автору курсу [Laurence Moroney  ](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction#instructors) а також Coursera та всім тим в Україні і у світі, хто надав можливіть викладачам проходити такі кльові курси безкоштовно.     

Усі мої звіти по цим курсам можете почитати за цим посиланням https://pupenasan.github.io/ml/blog/  

